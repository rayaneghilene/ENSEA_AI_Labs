{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rayaneghilene/ENSEA_AI_Labs/blob/main/Feed_Forward_Neural_Language_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 3, Part 2 : Feed Forward Neural Language Models"
      ],
      "metadata": {
        "id": "KaMsBkHIS_fY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJMQLZ_uf6tI"
      },
      "source": [
        "# About this lab\n",
        "\n",
        "In this session, you will experiment with feed-forward neural language models (FFLM) using [PyTorch](https://www.pytorch.org). To train the models, you will be using the [WikiText-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) corpus, which is a popular LM dataset introduced in 2016:\n",
        "\n",
        "> The WikiText language modeling dataset is a collection of texts extracted from Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License. Compared to the preprocessed version of Penn Treebank (PTB), `WikiText-2` is over 2 times larger. The dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.\n",
        "\n",
        "Goal of this lab : \n",
        "* Understand FFN\n",
        "* Train a FFNLM\n",
        "* Use PyTorch\n",
        "\n",
        "This part should take you 3h"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading Stuff & Setting Up the Environment"
      ],
      "metadata": {
        "id": "k5w6kOKHTYfB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eLd2J2B1i2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e9c97ee-192b-4390-dc21-76f624cc26bb"
      },
      "source": [
        "# Download the corpus\n",
        "%%bash\n",
        "URL=\"https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2\"\n",
        "\n",
        "for split in \"train\" \"valid\" \"test\"; do\n",
        "  if [ ! -f \"${split}.txt\" ]; then\n",
        "    echo \"Downloading ${split}.txt\"\n",
        "    wget -q \"${URL}/${split}.txt\"\n",
        "    # Remove empty lines\n",
        "    sed -i '/^ *$/d' \"${split}.txt\"\n",
        "    # Remove article titles starting with = and ending with =\n",
        "    sed -i '/^ *= .* = $/d' \"${split}\".txt\n",
        "  fi\n",
        "done\n",
        "\n",
        "# Prepare smaller version for fast training neural LMs\n",
        "head -n 5000 < train.txt > train_small.txt\n",
        "\n",
        "# Print the first 10 lines with line numbers\n",
        "cat -n train.txt | head -n10\n",
        "echo\n",
        "\n",
        "# Print some statistics\n",
        "echo -e \"\\n   Line,   word,   character counts\"\n",
        "wc *.txt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading train.txt\n",
            "Downloading valid.txt\n",
            "Downloading test.txt\n",
            "     1\t Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
            "     2\t The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n",
            "     3\t It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \n",
            "     4\t As with previous <unk> Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through <unk> text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely <unk> through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main <unk> , although they take a very minor role . \n",
            "     5\t The game 's battle system , the <unk> system , is carried over directly from <unk> Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters ' turns . Each character has a field and distance of movement limited by their Action <unk> . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant <unk> to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special <unk> that grant them temporary <unk> on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without <unk> his Action Point gauge , the character <unk> can shift into her \" Valkyria Form \" and become <unk> , while Imca can target multiple enemy units with her heavy weapon . \n",
            "     6\t Troops are divided into five classes : Scouts , <unk> , Engineers , <unk> and Armored Soldier . <unk> can switch classes by changing their assigned weapon . Changing class does not greatly affect the stats gained while in a previous class . With victory in battle , experience points are awarded to the squad , which are distributed into five different attributes shared by the entire squad , a feature differing from early games ' method of distributing to different unit types . \n",
            "     7\t The game takes place during the Second Europan War . Gallian Army Squad 422 , also known as \" The Nameless \" , are a penal military unit composed of criminals , foreign <unk> , and military offenders whose real names are erased from the records and <unk> officially referred to by numbers . <unk> by the Gallian military to perform the most dangerous missions that the Regular Army and Militia will not do , they are nevertheless up to the task , exemplified by their motto , <unk> <unk> , meaning \" Always Ready . \" The three main characters are <unk> Kurt Irving , an army officer falsely accused of treason who wishes to redeem himself ; Ace <unk> Imca , a female Darcsen heavy weapons specialist who seeks revenge against the Valkyria who destroyed her home ; and <unk> Riela <unk> , a seemingly <unk> young woman who is unknowingly a descendant of the Valkyria . Together with their fellow squad members , these three are tasked to fight against a mysterious Imperial unit known as Calamity Raven , consisting of mostly Darcsen soldiers . \n",
            "     8\t As the Nameless officially do not exist , the upper echelons of the Gallian Army exploit the concept of plausible <unk> in order to send them on missions that would otherwise make Gallia lose face in the war . While at times this works to their advantage , such as a successful incursion into Imperial territory , other orders cause certain members of the 422nd great distress . One such member , <unk> , becomes so enraged that he abandons his post and defects into the ranks of Calamity Raven , attached to the ideal of Darcsen independence proposed by their leader , Dahau . At the same time , elements within Gallian Army Command move to erase the Nameless in order to protect their own interests . <unk> by both allies and enemies , and combined with the presence of a traitor within their ranks , the 422nd desperately move to keep themselves alive while at the same time fight to help the Gallian war effort . This continues until the Nameless 's commanding officer , Ramsey Crowe , who had been kept under house arrest , is escorted to the capital city of <unk> in order to present evidence <unk> the weary soldiers and expose the real traitor , the Gallian General that had accused Kurt of Treason . \n",
            "     9\t <unk> due to these events , and partly due to the major losses in manpower Gallia suffers towards the end of the war with the Empire , the Nameless are offered a formal position as a squad in the Gallian Army rather than serve as an anonymous shadow force . This is short @-@ lived , however , as following Maximilian 's defeat , Dahau and Calamity Raven move to activate an ancient <unk> super weapon within the Empire , kept secret by their benefactor . Without the support of Maximilian or the chance to prove themselves in the war with Gallia , it is Dahau 's last <unk> card in creating a new Darcsen nation . As an armed Gallian force invading the Empire just following the two nations ' cease @-@ fire would certainly wreck their newfound peace , Kurt decides to once again make his squad the Nameless , asking Crowe to list himself and all under his command as killed @-@ in @-@ action . Now owing allegiance to none other than themselves , the 422nd confronts Dahau and destroys the <unk> weapon . Each member then goes their separate ways in order to begin their lives <unk> . \n",
            "    10\t Concept work for Valkyria Chronicles III began after development finished on Valkyria Chronicles II in early 2010 , with full development beginning shortly after this . The director of Valkyria Chronicles II , Takeshi Ozawa , returned to that role for Valkyria Chronicles III . Development work took approximately one year . After the release of Valkyria Chronicles II , the staff took a look at both the popular response for the game and what they wanted to do next for the series . Like its predecessor , Valkyria Chronicles III was developed for PlayStation Portable : this was due to the team wanting to refine the mechanics created for Valkyria Chronicles II , and they had not come up with the \" revolutionary \" idea that would warrant a new entry for the PlayStation 3 . Speaking in an interview , it was stated that the development team considered Valkyria Chronicles III to be the series ' first true sequel : while Valkyria Chronicles II had required a large amount of trial and error during development due to the platform move , the third game gave them a chance to improve upon the best parts of Valkyria Chronicles II due to being on the same platform . In addition to Sega staff from the previous games , development work was also handled by <unk> The original scenario was written <unk> <unk> , while the script was written by Hiroyuki <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> and <unk> <unk> . Its story was darker and more somber than that of its predecessor . \n",
            "\n",
            "\n",
            "   Line,   word,   character counts\n",
            "    2185   235854  1233015 test.txt\n",
            "    5000   558695  2954850 train_small.txt\n",
            "   17556  2007146 10596891 train.txt\n",
            "    1841   209338  1101534 valid.txt\n",
            "   26582  3011033 15886290 total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in order to allow deterministic behaviour, that is, make results reproducible\n",
        "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
      ],
      "metadata": {
        "id": "ZFy3NcUHP7Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee8e3ad-eb60-41c0-e6fd-935c0e3afaae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSFcmlFdf6tK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adfe62db-3a4f-47af-b077-3c858dc9cd2e"
      },
      "source": [
        "import math\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "# Fancy progress bar\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "###############\n",
        "# Torch setup #\n",
        "###############\n",
        "print('Torch version: {}, CUDA: {}'.format(torch.__version__, torch.version.cuda))\n",
        "cuda_available = torch.cuda.is_available()\n",
        "if not torch.cuda.is_available():\n",
        "  print('WARNING: You may want to change the runtime to GPU for Neural LM experiments!')\n",
        "  DEVICE = 'cpu'\n",
        "else:\n",
        "  DEVICE = 'cuda:0'\n",
        "\n",
        "#######################\n",
        "# Some helper functions\n",
        "#######################\n",
        "def fix_seed(seed=None):\n",
        "  \"\"\"Sets the seeds of random number generators.\"\"\"\n",
        "  torch.use_deterministic_algorithms(True)\n",
        "  if seed is None:\n",
        "    # Take a random seed\n",
        "    seed = time.time()\n",
        "  seed = int(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  return seed\n",
        "\n",
        "def readable_size(n):\n",
        "  \"\"\"Returns a readable size string for model parameters count.\"\"\"\n",
        "  sizes = ['K', 'M', 'G']\n",
        "  fmt = ''\n",
        "  size = n\n",
        "  for i, s in enumerate(sizes):\n",
        "    nn = n / (1000 ** (i + 1))\n",
        "    if nn >= 1:\n",
        "      size = nn\n",
        "      fmt = sizes[i]\n",
        "    else:\n",
        "      break\n",
        "  return '%.2f%s' % (size, fmt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.0.0+cu118, CUDA: 11.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmP0ZWbvB1aa"
      },
      "source": [
        "# Feed-forward Language Models (FFLM)\n",
        "\n",
        "FFLMs are similar to $n$-gram language models in the sense that the choice of $n$ is a hyperparameter for the network architecture. A basic FFLM constructs a  $C=n\\mathrm{-1}$ length context window before the word to be predicted. If the word embedding size is $E$, the feature vector for the context window becomes a vector of size $E\\times C$, resulting from the **concatenation** of individual word embeddings of context words. Hence, the choice of $C$ for FFLMs, affects the number of final learnable parameters in the network."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A - Dataset Stuff"
      ],
      "metadata": {
        "id": "c_sCtdDlUIvV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWMhsDZOf6tS"
      },
      "source": [
        "### Representing the vocabulary\n",
        "\n",
        "The below `Vocabulary` class encapsulates the **word-to-idx** and **idx-to-word** mapping that you should now be familiar with from the previous lab sessions. Read it to understand how the vocabulary is constructed from a plain text file, within the `build_from_file()` method. Special `<.>` markers are also included in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlxMOBzPf6tT"
      },
      "source": [
        "class Vocabulary(object):\n",
        "  \"\"\"Data structure representing the vocabulary of a corpus.\"\"\"\n",
        "  def __init__(self):\n",
        "    # Mapping from tokens to integers\n",
        "    self._word2idx = {}\n",
        "\n",
        "    # Reverse-mapping from integers to tokens\n",
        "    self.idx2word = []\n",
        "\n",
        "    # 0-padding token\n",
        "    self.add_word('<pad>')\n",
        "    # sentence start\n",
        "    self.add_word('<s>')\n",
        "    # sentence end\n",
        "    self.add_word('</s>')\n",
        "    # Unknown words\n",
        "    self.add_word('<unk>')\n",
        "\n",
        "    self._unk_idx = self._word2idx['<unk>']\n",
        "\n",
        "  def word2idx(self, word):\n",
        "    \"\"\"Returns the integer ID of the word or <unk> if not found.\"\"\"\n",
        "    return self._word2idx.get(word, self._unk_idx)\n",
        "\n",
        "  def add_word(self, word):\n",
        "    \"\"\"Adds the `word` into the vocabulary.\"\"\"\n",
        "    if word not in self._word2idx:\n",
        "      self.idx2word.append(word)\n",
        "      self._word2idx[word] = len(self.idx2word) - 1\n",
        "\n",
        "  def build_from_file(self, fname):\n",
        "    \"\"\"Builds a vocabulary from a given corpus file.\"\"\"\n",
        "    with open(fname) as f:\n",
        "      for line in f:\n",
        "        words = line.strip().split()\n",
        "        for word in words:\n",
        "          self.add_word(word)\n",
        "\n",
        "  def convert_idxs_to_words(self, idxs):\n",
        "    \"\"\"Converts a list of indices to words.\"\"\"\n",
        "    return ' '.join(self.idx2word[idx] for idx in idxs)\n",
        "\n",
        "  def convert_words_to_idxs(self, words):\n",
        "    \"\"\"Converts a list of words to a list of indices.\"\"\"\n",
        "    return [self.word2idx(w) for w in words]\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"Returns the size of the vocabulary.\"\"\"\n",
        "    return len(self.idx2word)\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return \"Vocabulary with {} items\".format(self.__len__())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hE1Lr6j_oYB"
      },
      "source": [
        "Let's construct the vocabulary for the training set and analyse the token indices for a sentence with an unknown word.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Why do we map unknown tokens to a special `<unk>` token?**\n",
        "When a model encounters an OOV word during testing, it cannot assign a probability to it because it has not seen that word before in the training data. This can lead to incorrect probability estimates and a higher perplexity.Additionally, mapping unknown tokens to a special token can help to reduce the sparsity of the vocabulary, making the model more robust to rare and unseen words.\n",
        "* **Do you think the network will learn a useful embedding for that? If not, how can you let the network to learn an embedding for it?**\n",
        "No, because <unk> token is not a meaningful word and doesn't have any semantic or syntactic context associated with it. We can also consider using pre-trained embeddings such as GloVe or fastText that have been trained on large amounts of text data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcNMhXqB5wwT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a235b57b-72d6-40bf-d1f5-72c6bc768454"
      },
      "source": [
        "vocab = Vocabulary()\n",
        "vocab.build_from_file('train.txt')\n",
        "print(vocab)\n",
        "\n",
        "# TODO : Convert sentence to list of indices, note how the last word is mapped to 3 (<unk>)\n",
        "sentence = \"<s> Get busy living, or get busy dying. </s>\"\n",
        "\n",
        "print(vocab.convert_words_to_idxs(sentence.split()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary with 33233 items\n",
            "[1, 11959, 6645, 3, 310, 4098, 6645, 3, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-4PQ9_8f6tV"
      },
      "source": [
        "### Representing the corpus\n",
        "\n",
        "Let's process the corpus for PyTorch: all splits will end up being a large, 1D token sequences. Note that, in `corpus_to_tensor()`, every line is wrapped between `<s> .. </s>` tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5s9MMt06YsL"
      },
      "source": [
        "def corpus_to_tensor(_vocab, filename):\n",
        "  # Final token indices\n",
        "  idxs = []  \n",
        "  with open(filename) as data:\n",
        "    for line in tqdm(data, ncols=80, unit=' line', desc=f'Reading {filename} '):\n",
        "      line = line.strip()\n",
        "      # Skip empty lines if any\n",
        "      if line:\n",
        "        # Each line is considered as a long sentence for WikiText-2\n",
        "        line = f\"<s> {line} </s>\"\n",
        "        # Split from whitespace and add sentence markers\n",
        "        idxs.extend(_vocab.convert_words_to_idxs(line.split()))\n",
        "  return torch.LongTensor(idxs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl6xuwZS9uO1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b15a3da-cb07-48c2-dd70-98b9cd045e27"
      },
      "source": [
        "# Read the files, prepare the small one as well\n",
        "train = corpus_to_tensor(vocab, 'train.txt')\n",
        "train_small = corpus_to_tensor(vocab, 'train_small.txt')\n",
        "\n",
        "valid = corpus_to_tensor(vocab, 'valid.txt')\n",
        "test = corpus_to_tensor(vocab, 'test.txt')\n",
        "print('\\n')\n",
        "\n",
        "print(f'Small training size in tokens: {readable_size(len(train_small))}')\n",
        "print(f'Training size in tokens: {readable_size(len(train))}')\n",
        "print(f'Validation size in tokens: {readable_size(len(valid))}')\n",
        "print(f'Test size in tokens: {readable_size(len(test))}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading train.txt : 17556 line [00:00, 21745.01 line/s]\n",
            "Reading train_small.txt : 5000 line [00:00, 23217.23 line/s]\n",
            "Reading valid.txt : 1841 line [00:00, 19509.87 line/s]\n",
            "Reading test.txt : 2185 line [00:00, 24267.52 line/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Small training size in tokens: 568.70K\n",
            "Training size in tokens: 2.04M\n",
            "Validation size in tokens: 213.02K\n",
            "Test size in tokens: 240.22K\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZclrKCDAjnlL",
        "outputId": "28a652b1-c191-4db1-e2b5-ef0a07bf8a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2042258])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNxNrdhPECML"
      },
      "source": [
        "**Q: Print the first 20 token indices from the training set. And then print the sentence in actual words corresponding to these 20 tokens by using one of the provided methods in the `Vocabulary` class.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXobO9_n-Giz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ced599b-df26-4686-cf70-2f20326f04a9"
      },
      "source": [
        "########\n",
        "# Answer\n",
        "########\n",
        "print(train[:20])\n",
        "print(vocab.convert_idxs_to_words(train[:20]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1,  4,  5,  6,  7,  8,  3,  9, 10, 11,  8, 12, 13, 14, 15,  6, 16, 17,\n",
            "        18,  7])\n",
            "<s> Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiroR-NszhGY"
      },
      "source": [
        "## B - Model definition\n",
        "\n",
        "Now that we are done with data loading and vocabulary construction, we can define the actual FFLM model in PyTorch. Recall from the lectures that this model requires a pre-defined context window size $C$ which will affect the way you set up some of the linear layers. **Note that**, in contrast to the model depicted in the lecture, this model has an additional layer `ff_ctx`, which projects the context vector $c_k$ to hidden dimension $H$. This ensures that the number of parameters in the output layer does not depend on the context size, i.e. it is always $H\\times V$ instead of $CE\\times V$.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Follow the comments in `__init__()` and `forward()` to fill in the missing parts with some actual code.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9ttYW2IC_UV"
      },
      "source": [
        "class FFLM(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_dim, hid_dim, context_size, dropout=0.5):\n",
        "    # Call parent's __init__ first\n",
        "    super(FFLM, self).__init__()\n",
        "    \n",
        "    # Store arguments\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_dim = emb_dim\n",
        "    self.hid_dim = hid_dim\n",
        "    self.context_size = context_size\n",
        "\n",
        "    # Create the loss, don't sum or average, we'll take care of it\n",
        "    # in the training loop for logging purposes\n",
        "    self.loss = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    # Create the non-linearity\n",
        "    self.nonlin = torch.nn.Tanh()\n",
        "\n",
        "    # Dropout regularizer\n",
        "    self.drop = nn.Dropout(p=dropout)\n",
        "\n",
        "    ##############################\n",
        "    # Fill the missing parts below\n",
        "    ##############################\n",
        "    # TODO : Compute the dimension of the context vector\n",
        "    self.context_dim = self.emb_dim * self.context_size\n",
        "    \n",
        "    # Create the embedding layer (i.e. lookup table tokens->vectors)\n",
        "    self.emb = nn.Embedding(\n",
        "        num_embeddings=self.vocab_size, embedding_dim=self.emb_dim,\n",
        "        padding_idx=0)\n",
        " \n",
        "    # This cuts the number of parameters a bit\n",
        "    self.ff_ctx = nn.Linear(self.context_dim, self.hid_dim)\n",
        "\n",
        "    ############################################\n",
        "    # Output layer mapping from the output of `ff_ctx` to vocabulary size\n",
        "    # TODO : Fill the dimensions of the output layer\n",
        "    ############################################\n",
        "    self.out =  nn.Linear(self.hid_dim, self.vocab_size)\n",
        "\n",
        "    # Purely for informational purposes: compute # of total params\n",
        "    self.n_params = 0\n",
        "    for param in self.parameters():\n",
        "        self.n_params += np.cumprod(param.data.size())[-1]\n",
        "    self.n_params = readable_size(self.n_params)\n",
        "      \n",
        "  def forward(self, x, y):\n",
        "    \"\"\"Forward-pass of the module.\"\"\"\n",
        "    # TODO : What is the shape of x ?\n",
        "    # x has shape (num_of_words)\n",
        "    # TODO : Get the embeddings for the token indices in `x`\n",
        "    batch_size = x.size()[0]\n",
        "    embs = self.emb(x)\n",
        "    # TODO : Concatenate the embeddings to form the context vector\n",
        "    ctx = embs.view(batch_size, -1)\n",
        "\n",
        "\n",
        "    # TODO : Apply ff_ctx -> non-lin -> dropout -> output layer to obtain the logits i.e. unnormalized scores   \n",
        "    logits = self.out(self.drop(self.nonlin(self.ff_ctx(ctx))))\n",
        "  \n",
        "    # TODO : Use self.loss to compute the losses, return the losses (true labels are in `y`)\n",
        "    return self.loss(logits.view(-1, logits.size(-1)), y)\n",
        "\n",
        "  def get_batches(self, data_tensor, batch_size=64):\n",
        "    \"\"\"Returns a tensor of size (n_batches, batch_size, context_size + 1).\"\"\"\n",
        "    # Split data into rows of n-grams followed by the (n+1)th true label\n",
        "    x_y = data_tensor.unfold(0, self.context_size + 1, step=1)\n",
        "\n",
        "    # Get the number of training n-grams\n",
        "    n_samples = x_y.size()[0]\n",
        "\n",
        "    # Hack: discard the last uneven batch for simplicity\n",
        "    n_batches = n_samples // batch_size\n",
        "    n_samples = n_batches * batch_size\n",
        "    # Split nicely into batches, i.e. (n_batches, batch_size, context_size + 1)\n",
        "    # The final element in each row is the ID of the true label to predict\n",
        "    x_y = x_y[:n_samples].view(n_batches, batch_size, -1)\n",
        "\n",
        "    # A particular batch for context_size=2 will now look like below in\n",
        "    # word format. Last element for every array is the next token to be predicted\n",
        "    #\n",
        "    # [[<s>, cat, sat],\n",
        "    #  [cat, sat, on],\n",
        "    #  [sat, on,  the],\n",
        "    #  [on,  the, mat],\n",
        "    #   ....\n",
        "    return x_y\n",
        "\n",
        "  def train_model(self, optim, train_tensor, valid_tensor, test_tensor, n_epochs=5,\n",
        "                 batch_size=64, shuffle=False):\n",
        "    \"\"\"Trains the model.\"\"\"\n",
        "    # Get batches for the training data\n",
        "    batches = self.get_batches(train_tensor, batch_size)\n",
        "    \n",
        "    print(f'Will do {batches.size(0)} batches for an epoch.')\n",
        "\n",
        "    for eidx in range(1, n_epochs + 1):\n",
        "      start_time = time.time()\n",
        "      epoch_loss = 0\n",
        "      epoch_items = 0\n",
        "\n",
        "      # Enable training mode\n",
        "      self.train()\n",
        "\n",
        "      # Shuffle the batch order or not\n",
        "      if shuffle:\n",
        "        batch_order = torch.randperm(batches.size(0))\n",
        "      else:\n",
        "        batch_order = torch.arange(batches.size(0))\n",
        "\n",
        "      # Start training\n",
        "      for iter_count, idx in enumerate(batch_order):\n",
        "        batch = batches[idx].to(DEVICE)\n",
        "\n",
        "        # TODO : Split into inputs `x` and labels `y`. Hint : Look at the context_size\n",
        "        x, y = batch[:, :-1], batch[:, -1]\n",
        "\n",
        "        # Clear the gradients\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # TODO : Compute the loss thanks to one of the previous function\n",
        "        \n",
        "        loss = self.forward(x,y)\n",
        "        \n",
        "        \n",
        "\n",
        "        # Backprop the average loss and update parameters\n",
        "        loss.mean().backward()\n",
        "        optim.step()\n",
        "\n",
        "        # sum the loss for reporting, along with the denominator\n",
        "        epoch_loss += loss.detach().sum()\n",
        "        epoch_items += loss.numel()\n",
        "\n",
        "        if iter_count % 1000 == 0:\n",
        "          # Print progress\n",
        "          loss_per_token = epoch_loss / epoch_items\n",
        "          ppl = math.exp(loss_per_token)\n",
        "          print(f'[Epoch {eidx:<3}] loss: {loss_per_token:6.2f}, perplexity: {ppl:6.2f}')\n",
        "\n",
        "      time_spent = time.time() - start_time\n",
        "\n",
        "      print(f'\\n[Epoch {eidx:<3}] ended with train_loss: {loss_per_token:6.2f}, ppl: {ppl:6.2f}')\n",
        "      # Evaluate on valid set\n",
        "      valid_loss, valid_ppl = self.evaluate(test_set=valid_tensor)\n",
        "      print(f'[Epoch {eidx:<3}] ended with valid_loss: {valid_loss:6.2f}, valid_ppl: {valid_ppl:6.2f}')\n",
        "      print(f'[Epoch {eidx:<3}] completed in {time_spent:.2f} seconds\\n')\n",
        "\n",
        "    # Evaluate the final model on test set\n",
        "    test_loss, test_ppl = self.evaluate(test_set=test_tensor)\n",
        "    print(f' ---> Final test set performance: {test_loss:6.2f}, test_ppl: {test_ppl:6.2f}')\n",
        "\n",
        "  def evaluate(self, test_set, batch_size=32):\n",
        "    \"\"\"Evaluates and computes perplexity for the given test set.\"\"\"\n",
        "    loss = 0\n",
        "\n",
        "    # Get the batches\n",
        "    batches = self.get_batches(test_set, batch_size)\n",
        "\n",
        "    # Set your model to Eval mode\n",
        "    self.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in batches:\n",
        "            batch = batch.to(DEVICE)\n",
        "\n",
        "            # Split into inputs `x` and labels `y`\n",
        "            x = batch[:, :-1]\n",
        "            y = batch[:, -1]\n",
        "\n",
        "            # Compute the loss for this batch\n",
        "            loss += self.forward(x, y).sum()\n",
        "\n",
        "    # Normalize by the number of tokens in the test set\n",
        "    loss /= batches.size()[:2].numel()\n",
        "\n",
        "    # Switch back to training mode\n",
        "    self.train()\n",
        "\n",
        "    # Return the perplexity and loss\n",
        "    return loss, math.exp(loss)\n",
        "\n",
        "\n",
        "  def __repr__(self):\n",
        "    \"\"\"String representation for pretty-printing.\"\"\"\n",
        "    s = super(FFLM, self).__repr__()\n",
        "    return f\"{s}\\n# of parameters: {self.n_params}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0eHpYiRUHDe"
      },
      "source": [
        "## C - Training\n",
        "\n",
        "We can now launch training using a set of sane hyper-parameters for our model. This is a 3-gram FFLM since the context size is set to 2. On a Colab GPU, a single epoch should take around 1 minute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rjwvEYYFjjE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c097419b-7e20-4233-c083-53df66252c4a"
      },
      "source": [
        "# Set the seed for reproducible results\n",
        "fix_seed(42)\n",
        "\n",
        "fflm_model = FFLM(\n",
        "    len(vocab),       # vocabulary size\n",
        "    emb_dim=128,      # word embedding dim\n",
        "    hid_dim=128,      # hidden layer dim\n",
        "    context_size=7,   # C = (N-1) if you think in n-gram LM terminology\n",
        "    dropout=0.3,      # dropout probability\n",
        ")\n",
        "print(len(vocab))\n",
        "\n",
        "# move to device\n",
        "fflm_model.to(DEVICE)\n",
        "\n",
        "# Initial learning rate for the optimizer\n",
        "FFLM_INIT_LR = 0.001\n",
        "\n",
        "# Create the optimizer\n",
        "fflm_optimizer = torch.optim.Adam(fflm_model.parameters(), lr=FFLM_INIT_LR)\n",
        "print(fflm_model)\n",
        "\n",
        "print('Starting training!')\n",
        "# NOTE: If you happen to have memory errors, try decreasing the batch size\n",
        "# It will print progress every 1000 batches\n",
        "fflm_model.train_model(fflm_optimizer, train, valid, test, n_epochs=5, batch_size=128, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33233\n",
            "FFLM(\n",
            "  (loss): CrossEntropyLoss()\n",
            "  (nonlin): Tanh()\n",
            "  (drop): Dropout(p=0.3, inplace=False)\n",
            "  (emb): Embedding(33233, 128, padding_idx=0)\n",
            "  (ff_ctx): Linear(in_features=896, out_features=128, bias=True)\n",
            "  (out): Linear(in_features=128, out_features=33233, bias=True)\n",
            ")\n",
            "# of parameters: 8.66M\n",
            "Starting training!\n",
            "Will do 15955 batches for an epoch.\n",
            "[Epoch 1  ] loss:  10.48, perplexity: 35606.58\n",
            "[Epoch 1  ] loss:   7.37, perplexity: 1583.40\n",
            "[Epoch 1  ] loss:   7.19, perplexity: 1329.13\n",
            "[Epoch 1  ] loss:   7.08, perplexity: 1188.81\n",
            "[Epoch 1  ] loss:   7.01, perplexity: 1109.15\n",
            "[Epoch 1  ] loss:   6.95, perplexity: 1046.94\n",
            "[Epoch 1  ] loss:   6.90, perplexity: 996.37\n",
            "[Epoch 1  ] loss:   6.86, perplexity: 957.72\n",
            "[Epoch 1  ] loss:   6.83, perplexity: 922.35\n",
            "[Epoch 1  ] loss:   6.79, perplexity: 892.86\n",
            "[Epoch 1  ] loss:   6.76, perplexity: 865.89\n",
            "[Epoch 1  ] loss:   6.73, perplexity: 840.95\n",
            "[Epoch 1  ] loss:   6.71, perplexity: 817.08\n",
            "[Epoch 1  ] loss:   6.68, perplexity: 796.57\n",
            "[Epoch 1  ] loss:   6.66, perplexity: 778.93\n",
            "[Epoch 1  ] loss:   6.64, perplexity: 762.98\n",
            "\n",
            "[Epoch 1  ] ended with train_loss:   6.64, ppl: 762.98\n",
            "[Epoch 1  ] ended with valid_loss:   5.89, valid_ppl: 363.21\n",
            "[Epoch 1  ] completed in 89.98 seconds\n",
            "\n",
            "[Epoch 2  ] loss:   6.38, perplexity: 590.70\n",
            "[Epoch 2  ] loss:   6.08, perplexity: 436.33\n",
            "[Epoch 2  ] loss:   6.08, perplexity: 437.15\n",
            "[Epoch 2  ] loss:   6.07, perplexity: 434.31\n",
            "[Epoch 2  ] loss:   6.08, perplexity: 435.44\n",
            "[Epoch 2  ] loss:   6.07, perplexity: 434.53\n",
            "[Epoch 2  ] loss:   6.08, perplexity: 435.21\n",
            "[Epoch 2  ] loss:   6.08, perplexity: 435.53\n",
            "[Epoch 2  ] loss:   6.08, perplexity: 436.80\n",
            "[Epoch 2  ] loss:   6.08, perplexity: 437.47\n",
            "[Epoch 2  ] loss:   6.08, perplexity: 436.68\n",
            "[Epoch 2  ] loss:   6.08, perplexity: 436.80\n",
            "[Epoch 2  ] loss:   6.08, perplexity: 436.61\n",
            "[Epoch 2  ] loss:   6.08, perplexity: 435.46\n",
            "[Epoch 2  ] loss:   6.07, perplexity: 434.85\n",
            "[Epoch 2  ] loss:   6.07, perplexity: 433.64\n",
            "\n",
            "[Epoch 2  ] ended with train_loss:   6.07, ppl: 433.64\n",
            "[Epoch 2  ] ended with valid_loss:   5.72, valid_ppl: 305.39\n",
            "[Epoch 2  ] completed in 88.15 seconds\n",
            "\n",
            "[Epoch 3  ] loss:   5.96, perplexity: 389.35\n",
            "[Epoch 3  ] loss:   5.76, perplexity: 317.64\n",
            "[Epoch 3  ] loss:   5.79, perplexity: 326.53\n",
            "[Epoch 3  ] loss:   5.80, perplexity: 330.00\n",
            "[Epoch 3  ] loss:   5.81, perplexity: 332.40\n",
            "[Epoch 3  ] loss:   5.81, perplexity: 334.22\n",
            "[Epoch 3  ] loss:   5.82, perplexity: 337.13\n",
            "[Epoch 3  ] loss:   5.82, perplexity: 338.19\n",
            "[Epoch 3  ] loss:   5.83, perplexity: 340.18\n",
            "[Epoch 3  ] loss:   5.83, perplexity: 341.98\n",
            "[Epoch 3  ] loss:   5.84, perplexity: 343.10\n",
            "[Epoch 3  ] loss:   5.84, perplexity: 344.19\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 345.67\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 347.38\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 348.52\n",
            "[Epoch 3  ] loss:   5.86, perplexity: 349.16\n",
            "\n",
            "[Epoch 3  ] ended with train_loss:   5.86, ppl: 349.16\n",
            "[Epoch 3  ] ended with valid_loss:   5.66, valid_ppl: 286.84\n",
            "[Epoch 3  ] completed in 88.34 seconds\n",
            "\n",
            "[Epoch 4  ] loss:   5.59, perplexity: 266.69\n",
            "[Epoch 4  ] loss:   5.58, perplexity: 264.20\n",
            "[Epoch 4  ] loss:   5.61, perplexity: 272.23\n",
            "[Epoch 4  ] loss:   5.62, perplexity: 274.55\n",
            "[Epoch 4  ] loss:   5.63, perplexity: 277.65\n",
            "[Epoch 4  ] loss:   5.64, perplexity: 281.47\n",
            "[Epoch 4  ] loss:   5.65, perplexity: 283.98\n",
            "[Epoch 4  ] loss:   5.66, perplexity: 286.46\n",
            "[Epoch 4  ] loss:   5.66, perplexity: 288.55\n",
            "[Epoch 4  ] loss:   5.67, perplexity: 290.49\n",
            "[Epoch 4  ] loss:   5.68, perplexity: 292.39\n",
            "[Epoch 4  ] loss:   5.69, perplexity: 294.98\n",
            "[Epoch 4  ] loss:   5.69, perplexity: 296.55\n",
            "[Epoch 4  ] loss:   5.70, perplexity: 297.97\n",
            "[Epoch 4  ] loss:   5.70, perplexity: 299.29\n",
            "[Epoch 4  ] loss:   5.70, perplexity: 300.31\n",
            "\n",
            "[Epoch 4  ] ended with train_loss:   5.70, ppl: 300.31\n",
            "[Epoch 4  ] ended with valid_loss:   5.59, valid_ppl: 268.43\n",
            "[Epoch 4  ] completed in 88.28 seconds\n",
            "\n",
            "[Epoch 5  ] loss:   6.05, perplexity: 424.71\n",
            "[Epoch 5  ] loss:   5.47, perplexity: 238.43\n",
            "[Epoch 5  ] loss:   5.48, perplexity: 240.02\n",
            "[Epoch 5  ] loss:   5.50, perplexity: 245.88\n",
            "[Epoch 5  ] loss:   5.51, perplexity: 248.08\n",
            "[Epoch 5  ] loss:   5.52, perplexity: 250.85\n",
            "[Epoch 5  ] loss:   5.53, perplexity: 253.20\n",
            "[Epoch 5  ] loss:   5.54, perplexity: 254.67\n",
            "[Epoch 5  ] loss:   5.55, perplexity: 257.11\n",
            "[Epoch 5  ] loss:   5.56, perplexity: 259.69\n",
            "[Epoch 5  ] loss:   5.57, perplexity: 262.13\n",
            "[Epoch 5  ] loss:   5.57, perplexity: 263.45\n",
            "[Epoch 5  ] loss:   5.58, perplexity: 265.22\n",
            "[Epoch 5  ] loss:   5.58, perplexity: 266.19\n",
            "[Epoch 5  ] loss:   5.59, perplexity: 267.68\n",
            "[Epoch 5  ] loss:   5.59, perplexity: 268.95\n",
            "\n",
            "[Epoch 5  ] ended with train_loss:   5.59, ppl: 268.95\n",
            "[Epoch 5  ] ended with valid_loss:   5.55, valid_ppl: 256.05\n",
            "[Epoch 5  ] completed in 88.19 seconds\n",
            "\n",
            " ---> Final test set performance:   5.47, test_ppl: 236.39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAHN-C-XHuVs"
      },
      "source": [
        "**Q: If everything goes well, you should see a loss of around ~10.4 printed as the first loss. This will still be the case if you change the random seed to some other number before model construction i.e. the culprit is not the exact values that they take.**\n",
        "* **Can you come up with a simple mathematical formula which yields that value?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiG_vI1uJj1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "333fab28-70f7-429d-f0eb-cf14242dd62c"
      },
      "source": [
        "##########################\n",
        "# Answer to question above\n",
        "##########################\n",
        "print(math.log(fflm_model.vocab_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.411298637141247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsYXRU68SWF7"
      },
      "source": [
        "## D - Further Exploring\n",
        "\n",
        "With the default settings above, you should end up with a validation perplexity of $\\sim1076$ and a final test set perplexity of $\\sim1003$ at the end of 5th epoch. Try the following questions to further analyze the model's prediction.\n",
        "\n",
        "---\n",
        "\n",
        "* **Q: Remove the `tanh()` non-linearity from the code so that the context is computed as a linear combination of its embeddings. How does the results compare to the initial one? Do you think non-linearity helps?**\n",
        "\n",
        "  **A: With the same hyperparameters, the non-linearity doesn't help that much the performance of the model: with thanh() test_ppl: 242.32 , without thanh() test_ppl: 251.98**\n",
        "* **Q: Compare the results by rerunning the training with unshuffled batches i.e. with `shuffle=False`. What do you notice in terms of results?**\n",
        "\n",
        "  **A: We notice a slightly better performance when running the traning with shuffled batches:  ---> Final test set performance:   5.49, test_ppl: 242.32, and for unshuffled batches:  ---> Final test set performance:   5.60, test_ppl: 269.26**\n",
        "\n",
        "* **Q: Play with hyper-parameters related to dimensions and dropout. Could you find a model with smaller perplexity?**\n",
        "\n",
        "    **A: hidden dimension changed to 256, result is: Final test set performance:   5.51, test_ppl: 247.81**\n",
        "    \n",
        "    **Best performance so far when we reduced the batch_size to 128, and the drop_out to 0.3:  ---> Final test set performance:   5.48, test_ppl: 237.39**\n",
        "\n",
        "    **Higher learning rate of 0.005 gives worse performance of : ---> Final test set performance:   5.93, test_ppl: 375.43**\n",
        "\n",
        "* **Q: Try with different context sizes such as 3, 5, 7, etc. What is the best perplexity you can get?**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "fflm_model = FFLM(\n",
        "    len(vocab),       # vocabulary size\n",
        "    emb_dim=128,      # word embedding dim\n",
        "    hid_dim=128,      # hidden layer dim\n",
        "    context_size=3,   # C = (N-1) if you think in n-gram LM terminology\n",
        "    dropout=0.3,      # dropout probability\n",
        ")\n",
        "\n",
        "# move to device\n",
        "fflm_model.to(DEVICE)\n",
        "\n",
        "# Initial learning rate for the optimizer\n",
        "FFLM_INIT_LR = 0.001\n",
        "\n",
        "# Create the optimizer\n",
        "fflm_optimizer = torch.optim.Adam(fflm_model.parameters(), lr=FFLM_INIT_LR)\n",
        "print(fflm_model)\n",
        "\n",
        "print('Starting training!')\n",
        "# NOTE: If you happen to have memory errors, try decreasing the batch size\n",
        "# It will print progress every 1000 batches\n",
        "fflm_model.train_model(fflm_optimizer, train, valid, test, n_epochs=5, batch_size=128, shuffle=True)\n",
        "```\n",
        "result is :  ---> Final test set performance:   5.44, test_ppl: 230.66\n",
        "\n",
        "with a context size of 5\n",
        "\n",
        "result is :  ---> Final test set performance:   5.43, test_ppl: 229.18\n",
        "\n",
        "with a context size of 7 \n",
        " \n",
        "result is :  ---> Final test set performance:   5.48, test_ppl: 239.34"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  **Without the `tanh()`**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l8e3Vds9RsUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the same hyperparameters, the non-linearity doesn't help that much the performance of the model: with thanh() test_ppl: 242.32 , without thanh() test_ppl: 251.98"
      ],
      "metadata": {
        "id": "JqV4V0F-TqNY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq1fn-x9aS5e"
      },
      "source": [
        "## E - Further Reading for your knowledge\n",
        " - [Original FFLM paper from Bengio et al. 2003](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
        " - [Original RNNLM paper from Mikolov et al. 2010](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)\n",
        " - Some recent state-of-the-art LSTM-based RNNLMs\n",
        "\n",
        "  - [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/pdf/1708.02182.pdf)\n",
        "  - [An Analysis of Neural Language Modeling at Multiple Scales](https://arxiv.org/pdf/1803.08240.pdf)\n",
        "  - [Scalable Language Modeling: WikiText-103 on a Single GPU in 12 hours](https://mlsys.org/Conferences/2019/doc/2018/50.pdf)"
      ]
    }
  ]
}